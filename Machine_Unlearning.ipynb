{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### <b>Downloading and importing libraries</b>"
      ],
      "metadata": {
        "id": "aK9x4CGWpMsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download\n",
        "# %pip install pillow\n",
        "# %pip install matplotlib\n",
        "# %pip install numpy\n",
        "# %pip install pandas\n",
        "# %pip install torch\n",
        "# %pip install torchvision\n",
        "# %pip install scikit-learn"
      ],
      "metadata": {
        "id": "yqnv6aFKim68"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qJl1GEjIDTuA",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import glob\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms, models\n",
        "import torchvision.transforms.functional as TF\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from sklearn import linear_model, model_selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XQByno9CIUg"
      },
      "source": [
        "#### <b>Download the Dataset</b>\n",
        "\n",
        "* The dataset contains approximately 13,000 Korean \"human face\" images.\n",
        "* In this dataset, all faces are cropped to a resolution of 128 X 128 pixels, although some of the original images have been high resolution.\n",
        "* Each image filename indicates which family (household) number it belongs to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UWvuBOPACCza"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!wget https://postechackr-my.sharepoint.com/:u:/g/personal/dongbinna_postech_ac_kr/EbMhBPnmIb5MutZvGicPKggBWKm5hLs0iwKfGW7_TwQIKg?download=1 -O custom_korean_family_dataset_resolution_128.zip\n",
        "!unzip custom_korean_family_dataset_resolution_128.zip -d ./custom_korean_family_dataset_resolution_128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwI1XMnIg7Z_"
      },
      "source": [
        "### Handling uploaded image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# choose how to forget\n",
        "only_upload_data= False"
      ],
      "metadata": {
        "id": "Xrit-SozjTnH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "BcMe3_FEg7Z_"
      },
      "outputs": [],
      "source": [
        "# Define the path and label for your new image\n",
        "upload_path = \"/content/custom_korean_family_dataset_resolution_128/uploads\"\n",
        "upload_label = 'd' # replace with the actual label\n",
        "upload_file_name = \"F0001_AGE_M_45_d1.jpg\"  # Replace with your actual file name\n",
        "\n",
        "# Create the DataFrame\n",
        "upload_dataset = pd.DataFrame({\n",
        "    'image_path': [upload_file_name],\n",
        "    'age_class': [upload_label]\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkbDGUSECOC7"
      },
      "source": [
        "#### <b>Load Datasets</b>\n",
        "\n",
        "* The following three datasets do not overlap at the household level.\n",
        "  * Thus, our setting ensures any individual \"subjects\" are overlapped across the following three sub-dataset.\n",
        "  * In other wrods, we set any person (subject) to do not simultaneously belonging to the $\\mathcal{D}_{train}$, $\\mathcal{D}_{test}$ and $\\mathcal{D}_{unseen}$.\n",
        "* <b>Training dataset $\\mathcal{D}_{train}$</b>: (F0001 ~ F0299) folders have 10,025 images.\n",
        "* <b>Test dataset $\\mathcal{D}_{test}$</b>: (F0801 ~ F0850) folders have 1,539 images.\n",
        "* <b>Unseen dataset $\\mathcal{D}_{unseen}$</b>: (F0851 ~ F0900) folders have 1,504 images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Be8TVf3nDOBF",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def parsing(meta_data):\n",
        "    image_age_list = []\n",
        "    # iterate all rows in the metadata file\n",
        "    for idx, row in meta_data.iterrows():\n",
        "        image_path = row['image_path']\n",
        "        age_class = row['age_class']\n",
        "        image_age_list.append([image_path, age_class])\n",
        "    return image_age_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KTvEPlug7aB"
      },
      "source": [
        "## modified"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "EAA5k6Wkg7aC"
      },
      "outputs": [],
      "source": [
        "# Modified DATASET class\n",
        "class Dataset(Dataset):\n",
        "    def __init__(self, meta_data,image_directory, transform=None, forget=False, retain=False , forget_data=upload_dataset, only_upload_forget=only_upload_data):\n",
        "        self.meta_data = meta_data\n",
        "        self.image_directory = image_directory\n",
        "        self.transform = transform\n",
        "\n",
        "        # Process the metadata.\n",
        "        image_age_list = parsing(meta_data)\n",
        "        forget_list = parsing(forget_data)\n",
        "\n",
        "        self.forget_list = forget_list\n",
        "        self.image_age_list = image_age_list\n",
        "        self.age_class_to_label = {\n",
        "            \"a\": 0, \"b\": 1, \"c\": 2, \"d\": 3, \"e\": 4, \"f\": 5, \"g\": 6, \"h\": 7\n",
        "        }\n",
        "\n",
        "        # After training the original model, we will do \"machine unlearning\".\n",
        "        # The machine unlearning requires two datasets, ① forget dataset and ② retain dataset.\n",
        "        # In this experiment, we set the first 1,500 images to be forgotten and the rest images to be retained.\n",
        "        if forget:\n",
        "            if only_upload_forget:\n",
        "                self.image_age_list = self.forget_list\n",
        "            else:\n",
        "                self.image_age_list = self.image_age_list[:1500] + self.forget_list\n",
        "        if retain:\n",
        "            self.image_age_list = self.image_age_list[1500:]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_age_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path, age_class = self.image_age_list[idx]\n",
        "        if image_path == upload_file_name:\n",
        "          img = Image.open(os.path.join(upload_path, image_path))\n",
        "        else:\n",
        "          img = Image.open(os.path.join(self.image_directory, image_path))\n",
        "        label = self.age_class_to_label[age_class]\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "M7giqCBhGsnJ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "label_to_age = {\n",
        "    0: \"0-6 years old\",\n",
        "    1: \"7-12 years old\",\n",
        "    2: \"13-19 years old\",\n",
        "    3: \"20-30 years old\",\n",
        "    4: \"31-45 years old\",\n",
        "    5: \"46-55 years old\",\n",
        "    6: \"56-66 years old\",\n",
        "    7: \"67-80 years old\"\n",
        "}\n",
        "\n",
        "train_meta_data_path = \"./custom_korean_family_dataset_resolution_128/custom_train_dataset.csv\"\n",
        "train_meta_data = pd.read_csv(train_meta_data_path)\n",
        "train_image_directory = \"./custom_korean_family_dataset_resolution_128/train_images\"\n",
        "\n",
        "test_meta_data_path = \"./custom_korean_family_dataset_resolution_128/custom_val_dataset.csv\"\n",
        "test_meta_data = pd.read_csv(test_meta_data_path)\n",
        "test_image_directory = \"./custom_korean_family_dataset_resolution_128/val_images\"\n",
        "\n",
        "unseen_meta_data_path = \"./custom_korean_family_dataset_resolution_128/custom_test_dataset.csv\"\n",
        "unseen_meta_data = pd.read_csv(unseen_meta_data_path)\n",
        "unseen_image_directory = \"./custom_korean_family_dataset_resolution_128/test_images\"\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(128),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomAffine(0, shear=10, scale=(0.8, 1.2)),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(128),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "unseen_transform = transforms.Compose([\n",
        "    transforms.Resize(128),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_dataset = Dataset(train_meta_data, train_image_directory, train_transform)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "test_dataset = Dataset(test_meta_data, test_image_directory, test_transform)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "unseen_dataset = Dataset(unseen_meta_data, unseen_image_directory, unseen_transform)\n",
        "unseen_dataloader = DataLoader(unseen_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nck1gGsvITcS"
      },
      "source": [
        "#### <b>Train the \"Original Model\"</b>\n",
        "\n",
        "* Train the Original model to serve as the base model for performing Machine Unlearning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-qsC5RlYIJwh",
        "tags": []
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.01\n",
        "log_step = 30\n",
        "\n",
        "model = models.resnet18(pretrained=False)\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Linear(num_features, 8)\n",
        "model = model.cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "sf_vtq7uIjWH",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    start_time = time.time()\n",
        "    print(f'[Epoch: {epoch + 1} - Training]')\n",
        "    model.train()\n",
        "    total = 0\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    for i, batch in enumerate(train_dataloader):\n",
        "        imgs, labels = batch\n",
        "        imgs, labels = imgs.cuda(), labels.cuda()\n",
        "\n",
        "        outputs = model(imgs)\n",
        "        optimizer.zero_grad()\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total += labels.shape[0]\n",
        "        running_loss += loss.item()\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        if i % log_step == log_step - 1:\n",
        "            print(f'[Batch: {i + 1}] running train loss: {running_loss / total}, running train accuracy: {running_corrects / total}')\n",
        "\n",
        "    print(f'train loss: {running_loss / total}, accuracy: {running_corrects / total}')\n",
        "    print(\"elapsed time:\", time.time() - start_time)\n",
        "    return running_loss / total, (running_corrects / total).item()\n",
        "\n",
        "\n",
        "def test():\n",
        "    start_time = time.time()\n",
        "    print(f'[Test]')\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    for i, batch in enumerate(test_dataloader):\n",
        "        imgs, labels = batch\n",
        "        imgs, labels = imgs.cuda(), labels.cuda()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(imgs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        total += labels.shape[0]\n",
        "        running_loss += loss.item()\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        if (i == 0) or (i % log_step == log_step - 1):\n",
        "            print(f'[Batch: {i + 1}] running test loss: {running_loss / total}, running test accuracy: {running_corrects / total}')\n",
        "\n",
        "    print(f'test loss: {running_loss / total}, accuracy: {running_corrects / total}')\n",
        "    print(\"elapsed time:\", time.time() - start_time)\n",
        "    return running_loss / total, (running_corrects / total).item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "pzstS9NMKeGA",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    lr = learning_rate\n",
        "    if epoch >= 10:\n",
        "        lr /= 10\n",
        "    if epoch >= 20:\n",
        "        lr /= 10\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "num_original_epochs = 30\n",
        "best_test_acc = 0\n",
        "best_epoch = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXKsCFIXlY3m",
        "tags": []
      },
      "outputs": [],
      "source": [
        "history = []\n",
        "accuracy = []\n",
        "for epoch in range(num_original_epochs):\n",
        "    adjust_learning_rate(optimizer, epoch)\n",
        "    train_loss, train_acc = train()\n",
        "    test_loss, test_acc = test()\n",
        "    history.append((train_loss, test_loss))\n",
        "    accuracy.append((train_acc, test_acc))\n",
        "\n",
        "    if test_acc > best_test_acc:\n",
        "        print(\"[Info] best test accuracy!\")\n",
        "        best_test_acc = test_acc\n",
        "        best_epoch = epoch\n",
        "        torch.save(model.state_dict(), f'best_checkpoint_epoch_{epoch + 1}.pth')\n",
        "\n",
        "torch.save(model.state_dict(), f'last_checkpoint_epoch_{num_original_epochs}.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVPr67OImRWL"
      },
      "source": [
        "#### <b>(Option) Load the Pre-trained Original Model</b>\n",
        "\n",
        "* We can easily download the pre-trained original model, rather than training the original model from scratch.\n",
        "* <b>Download Link</b>: https://postechackr-my.sharepoint.com/:u:/g/personal/dongbinna_postech_ac_kr/EfNlaO0X8_1Ij4WPPLs09cgBQl3WB_y7zNTNwvRvIn2CDg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O pre_trained_last_checkpoint_epoch_30.pth \"https://postechackr-my.sharepoint.com/:u:/g/personal/dongbinna_postech_ac_kr/EfNlaO0X8_1Ij4WPPLs09cgBQl3WB_y7zNTNwvRvIn2CDg?download=1\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5Ojf4a4tgTC",
        "outputId": "66c1519e-1d84-4d9e-ebdd-31c45dc7daec"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-02 22:46:23--  https://postechackr-my.sharepoint.com/:u:/g/personal/dongbinna_postech_ac_kr/EfNlaO0X8_1Ij4WPPLs09cgBQl3WB_y7zNTNwvRvIn2CDg?download=1\n",
            "Resolving postechackr-my.sharepoint.com (postechackr-my.sharepoint.com)... 13.107.136.10, 13.107.138.10, 2620:1ec:8f8::10, ...\n",
            "Connecting to postechackr-my.sharepoint.com (postechackr-my.sharepoint.com)|13.107.136.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /personal/dongbinna_postech_ac_kr/Documents/Research/models/Machine%20Unlearning/last_checkpoint_epoch_30.pth?ga=1 [following]\n",
            "--2023-12-02 22:46:25--  https://postechackr-my.sharepoint.com/personal/dongbinna_postech_ac_kr/Documents/Research/models/Machine%20Unlearning/last_checkpoint_epoch_30.pth?ga=1\n",
            "Reusing existing connection to postechackr-my.sharepoint.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 44803358 (43M) [application/octet-stream]\n",
            "Saving to: ‘pre_trained_last_checkpoint_epoch_30.pth’\n",
            "\n",
            "pre_trained_last_ch 100%[===================>]  42.73M  9.40MB/s    in 4.5s    \n",
            "\n",
            "2023-12-02 22:46:30 (9.40 MB/s) - ‘pre_trained_last_checkpoint_epoch_30.pth’ saved [44803358/44803358]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khmqXBaHQIDX"
      },
      "source": [
        "<b>Final Test</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cmAUsZsQJM2",
        "outputId": "814af236-dad2-4b98-ef14-29f0743272ae",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Test]\n",
            "[Batch: 1] running test loss: 0.02325533702969551, running test accuracy: 0.65625\n",
            "test loss: 0.026779032899932168, accuracy: 0.5951917171478271\n",
            "elapsed time: 2.9904704093933105\n",
            "test loss: 0.026779032899932168\n",
            "test acc: 0.5951917171478271\n"
          ]
        }
      ],
      "source": [
        "model = models.resnet18(pretrained=False)\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Linear(num_features, 8)\n",
        "model = model.cuda()\n",
        "# model_path = f'last_checkpoint_epoch_{num_original_epochs}.pth' # If you trian the original model from scratch.\n",
        "model_path = '/content/pre_trained_last_checkpoint_epoch_30.pth'\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "test_loss, test_acc = test()\n",
        "print(\"test loss:\", test_loss)\n",
        "print(\"test acc:\", test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GomPCiAKg7aQ"
      },
      "source": [
        "## Modified"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "ZaJPDiVlg7aR"
      },
      "outputs": [],
      "source": [
        "if only_upload_data:\n",
        "    forget_dataset_train = Dataset(train_meta_data, train_image_directory, train_transform, forget=True)\n",
        "    forget_dataloader_train = DataLoader(forget_dataset_train, batch_size=1, shuffle=False)\n",
        "\n",
        "    forget_dataset_test = Dataset(train_meta_data, train_image_directory, test_transform, forget=True)\n",
        "    forget_dataloader_test = DataLoader(forget_dataset_test, batch_size=1, shuffle=False)\n",
        "else:\n",
        "    forget_dataset_train = Dataset(train_meta_data, train_image_directory, train_transform, forget=True)\n",
        "    forget_dataloader_train = DataLoader(forget_dataset_train, batch_size=64, shuffle=True)\n",
        "\n",
        "    forget_dataset_test = Dataset(train_meta_data, train_image_directory, test_transform, forget=True)\n",
        "    forget_dataloader_test = DataLoader(forget_dataset_test, batch_size=64, shuffle=False)\n",
        "\n",
        "retain_dataset_train = Dataset(train_meta_data, train_image_directory, train_transform, retain=True)\n",
        "retain_dataloader_train = DataLoader(retain_dataset_train, batch_size=64, shuffle=True)\n",
        "\n",
        "retain_dataset_test = Dataset(train_meta_data, train_image_directory, test_transform, retain=True)\n",
        "retain_dataloader_test = DataLoader(retain_dataset_test, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Vp1S7RFg7aR",
        "outputId": "fe1d5dcc-d508-4b30-a914-9d8d8d8c66bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset size: 10025\n",
            "Test dataset size: 1539\n",
            "Forget dataset size: 1501\n",
            "Retain dataset size: 8525\n",
            "Unseen dataset size: 1504\n"
          ]
        }
      ],
      "source": [
        "print('Train dataset size:', len(train_dataset))\n",
        "print('Test dataset size:', len(test_dataset))\n",
        "print('Forget dataset size:', len(forget_dataset_train))\n",
        "print('Retain dataset size:', len(retain_dataset_train))\n",
        "print('Unseen dataset size:', len(unseen_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "AGI6rxao3RKQ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluation(model, data_loader):\n",
        "    start_time = time.time()\n",
        "    print(f'[Test]')\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    running_top2_corrects = 0\n",
        "    log_step = 20\n",
        "\n",
        "    for i, batch in enumerate(data_loader):\n",
        "        imgs, labels = batch\n",
        "        imgs, labels = imgs.cuda(), labels.cuda()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(imgs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Top-2 accuracy.\n",
        "            _, top2_preds = outputs.topk(2, dim=1)  # Get the top 2 class indices.\n",
        "            top2_correct = top2_preds.eq(labels.view(-1, 1).expand_as(top2_preds))\n",
        "            running_top2_corrects += top2_correct.any(dim=1).sum().item()\n",
        "\n",
        "        total += labels.shape[0]\n",
        "        running_loss += loss.item()\n",
        "        running_corrects += torch.sum(preds == labels.data).item()\n",
        "\n",
        "        if (i == 0) or (i % log_step == log_step - 1):\n",
        "            print(f'[Batch: {i + 1}] running test loss: {running_loss / total}, running test accuracy: {running_corrects / total}, running top-2 accuracy: {running_top2_corrects / total}')\n",
        "\n",
        "    print(f'test loss: {running_loss / total}, accuracy: {running_corrects / total}, top-2 accuracy: {running_top2_corrects / total}')\n",
        "    print(\"elapsed time:\", time.time() - start_time)\n",
        "    return {'Loss': running_loss / total, 'Acc': running_corrects / total, 'Top-2 Acc': running_top2_corrects / total}\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "PX5rU0Tb2gxE",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def compute_losses(net, loader):\n",
        "    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
        "    all_losses = []\n",
        "\n",
        "    for inputs, y in loader:\n",
        "        targets = y\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "        logits = net(inputs)\n",
        "\n",
        "        losses = criterion(logits, targets).cpu().detach().numpy()\n",
        "        for l in losses:\n",
        "            all_losses.append(l)\n",
        "\n",
        "    return np.array(all_losses)\n",
        "\n",
        "def simple_mia(sample_loss, members, n_splits=10, random_state=0):\n",
        "    unique_members = np.unique(members)\n",
        "    if not np.all(unique_members == np.array([0, 1])):\n",
        "        raise ValueError(\"members should only have 0 and 1s\")\n",
        "\n",
        "    attack_model = linear_model.LogisticRegression()\n",
        "    cv = model_selection.StratifiedShuffleSplit(\n",
        "        n_splits=n_splits, random_state=random_state\n",
        "    )\n",
        "    return model_selection.cross_val_score(\n",
        "        attack_model, sample_loss, members, cv=cv, scoring=\"accuracy\"\n",
        "    )\n",
        "\n",
        "def cal_mia(model):\n",
        "    set_seed(42)\n",
        "\n",
        "    forget_losses = compute_losses(model, forget_dataloader_test)\n",
        "    unseen_losses = compute_losses(model, unseen_dataloader)\n",
        "\n",
        "    np.random.shuffle(forget_losses)\n",
        "    forget_losses = forget_losses[: len(unseen_losses)]\n",
        "\n",
        "    samples_mia = np.concatenate((unseen_losses, forget_losses)).reshape((-1, 1))\n",
        "    labels_mia = [0] * len(unseen_losses) + [1] * len(forget_losses)\n",
        "\n",
        "    mia_scores = simple_mia(samples_mia, labels_mia)\n",
        "    forgetting_score = abs(0.5 - mia_scores.mean())\n",
        "\n",
        "    return {'MIA': mia_scores.mean(), 'Forgeting Score': forgetting_score}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Original Model Preformance</b>"
      ],
      "metadata": {
        "id": "Xu6i6YH20t2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print(f'last_checkpoint_epoch_{num_original_epochs}.pth')\n",
        "# original_save_path = f'last_checkpoint_epoch_{num_original_epochs}.pth' # If you trian the original model from scratch.\n",
        "original_save_path = '/content/pre_trained_last_checkpoint_epoch_30.pth'\n",
        "original_model = models.resnet18(pretrained=False)\n",
        "num_features = original_model.fc.in_features\n",
        "original_model.fc = nn.Linear(num_features, 8)\n",
        "original_model.load_state_dict(torch.load(original_save_path))\n",
        "original_model = original_model.cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "test_acc = evaluation(original_model, test_dataloader)\n",
        "test_acc\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Performance\n",
        "test_acc = evaluation(original_model, test_dataloader)\n",
        "unseen_acc = evaluation(original_model, unseen_dataloader)\n",
        "mia = cal_mia(original_model.cuda())\n",
        "print()\n",
        "print(f'Test Acc: {test_acc}')\n",
        "print(f'Unseen Acc: {unseen_acc}')\n",
        "print(f'MIA: {mia}')\n",
        "print(f'Final Score: {(test_acc[\"Acc\"] + (1 - abs(mia[\"MIA\"] - 0.5) * 2)) / 2}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRr-g6b6zlbb",
        "outputId": "82ad137b-c667-48ba-ec72-d74cae47516a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Test]\n",
            "[Batch: 1] running test loss: 0.02325533702969551, running test accuracy: 0.65625, running top-2 accuracy: 0.9375\n",
            "[Batch: 20] running test loss: 0.024291572719812395, running test accuracy: 0.6015625, running top-2 accuracy: 0.88359375\n",
            "test loss: 0.026779032899932168, accuracy: 0.5951916829109811, top-2 accuracy: 0.8804418453541261\n",
            "elapsed time: 1.6101903915405273\n",
            "[Test]\n",
            "[Batch: 1] running test loss: 0.02325533702969551, running test accuracy: 0.65625, running top-2 accuracy: 0.9375\n",
            "[Batch: 20] running test loss: 0.024291572719812395, running test accuracy: 0.6015625, running top-2 accuracy: 0.88359375\n",
            "test loss: 0.026779032899932168, accuracy: 0.5951916829109811, top-2 accuracy: 0.8804418453541261\n",
            "elapsed time: 1.583770990371704\n",
            "[Test]\n",
            "[Batch: 1] running test loss: 0.019372565671801567, running test accuracy: 0.640625, running top-2 accuracy: 0.9375\n",
            "[Batch: 20] running test loss: 0.020526642585173248, running test accuracy: 0.65703125, running top-2 accuracy: 0.89609375\n",
            "test loss: 0.021540874337896386, accuracy: 0.65625, top-2 accuracy: 0.8936170212765957\n",
            "elapsed time: 1.5423359870910645\n",
            "\n",
            "Test Acc: {'Loss': 0.026779032899932168, 'Acc': 0.5951916829109811, 'Top-2 Acc': 0.8804418453541261}\n",
            "Unseen Acc: {'Loss': 0.021540874337896386, 'Acc': 0.65625, 'Top-2 Acc': 0.8936170212765957}\n",
            "MIA: {'MIA': 0.7102990033222591, 'Forgeting Score': 0.21029900332225915}\n",
            "Final Score: 0.5872968381332314\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <b>Negative Gradient Ascent</b>\n",
        "* A strategy that induces the model to 'forget' particular data by deliberately maximizing the error on the forget dataset."
      ],
      "metadata": {
        "id": "cn3G8Pnl0vYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print(f'last_checkpoint_epoch_{num_original_epochs}.pth')\n",
        "# original_save_path = f'last_checkpoint_epoch_{num_original_epochs}.pth' # If you trian the original model from scratch.\n",
        "original_save_path = '/content/pre_trained_last_checkpoint_epoch_30.pth'\n",
        "unlearned_model = models.resnet18(pretrained=False)\n",
        "num_features = unlearned_model.fc.in_features\n",
        "unlearned_model.fc = nn.Linear(num_features, 8)\n",
        "unlearned_model.load_state_dict(torch.load(original_save_path))\n",
        "unlearned_model = unlearned_model.cuda()\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "iCtx3y_L0zMe"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(unlearned_model.parameters(), lr=0.001)\n",
        "\n",
        "dataloader_iterator = iter(forget_dataloader_train)\n",
        "\n",
        "num_epochs = 2\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0\n",
        "\n",
        "    for batch_idx, (x_retain, y_retain) in enumerate(retain_dataloader_train):\n",
        "        y_retain = y_retain.cuda()\n",
        "\n",
        "        try:\n",
        "            (x_forget, y_forget) = next(dataloader_iterator)\n",
        "        except StopIteration:\n",
        "            dataloader_iterator = iter(forget_dataloader_train)\n",
        "            (x_forget, y_forget) = next(dataloader_iterator)\n",
        "\n",
        "        if x_forget.size(0) != x_retain.size(0):\n",
        "            continue\n",
        "\n",
        "        outputs_forget = unlearned_model(x_forget.cuda())\n",
        "        loss_ascent_forget = -criterion(outputs_forget, y_forget.cuda())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss_ascent_forget.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss_ascent_forget.item() * x_retain.size(0)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(retain_dataloader_train)}] - Batch Loss: {loss_ascent_forget.item():.4f}\")\n",
        "\n",
        "    average_epoch_loss = running_loss / (len(retain_dataloader_train) * x_retain.size(0))\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Total Loss: {running_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGiBlOW61AGd",
        "outputId": "29a80aed-7a44-4cf8-9cc6-0de3f9e8464d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/2], Batch [1/134] - Batch Loss: -0.2433\n",
            "Epoch [1/2], Batch [2/134] - Batch Loss: -0.0660\n",
            "Epoch [1/2], Batch [3/134] - Batch Loss: -0.0934\n",
            "Epoch [1/2], Batch [4/134] - Batch Loss: -0.1081\n",
            "Epoch [1/2], Batch [5/134] - Batch Loss: -0.0712\n",
            "Epoch [1/2], Batch [6/134] - Batch Loss: -0.0679\n",
            "Epoch [1/2], Batch [7/134] - Batch Loss: -0.0626\n",
            "Epoch [1/2], Batch [8/134] - Batch Loss: -0.2756\n",
            "Epoch [1/2], Batch [9/134] - Batch Loss: -0.1774\n",
            "Epoch [1/2], Batch [10/134] - Batch Loss: -0.2316\n",
            "Epoch [1/2], Batch [11/134] - Batch Loss: -0.1281\n",
            "Epoch [1/2], Batch [12/134] - Batch Loss: -0.1752\n",
            "Epoch [1/2], Batch [13/134] - Batch Loss: -0.1325\n",
            "Epoch [1/2], Batch [14/134] - Batch Loss: -0.1533\n",
            "Epoch [1/2], Batch [15/134] - Batch Loss: -0.1743\n",
            "Epoch [1/2], Batch [16/134] - Batch Loss: -0.0749\n",
            "Epoch [1/2], Batch [17/134] - Batch Loss: -0.0861\n",
            "Epoch [1/2], Batch [18/134] - Batch Loss: -0.1710\n",
            "Epoch [1/2], Batch [19/134] - Batch Loss: -0.1403\n",
            "Epoch [1/2], Batch [20/134] - Batch Loss: -0.1174\n",
            "Epoch [1/2], Batch [21/134] - Batch Loss: -0.1242\n",
            "Epoch [1/2], Batch [22/134] - Batch Loss: -0.0813\n",
            "Epoch [1/2], Batch [23/134] - Batch Loss: -0.0894\n",
            "Epoch [1/2], Batch [25/134] - Batch Loss: -0.1632\n",
            "Epoch [1/2], Batch [26/134] - Batch Loss: -0.1725\n",
            "Epoch [1/2], Batch [27/134] - Batch Loss: -0.2913\n",
            "Epoch [1/2], Batch [28/134] - Batch Loss: -0.0688\n",
            "Epoch [1/2], Batch [29/134] - Batch Loss: -0.1315\n",
            "Epoch [1/2], Batch [30/134] - Batch Loss: -0.0478\n",
            "Epoch [1/2], Batch [31/134] - Batch Loss: -0.1158\n",
            "Epoch [1/2], Batch [32/134] - Batch Loss: -0.1288\n",
            "Epoch [1/2], Batch [33/134] - Batch Loss: -0.1507\n",
            "Epoch [1/2], Batch [34/134] - Batch Loss: -0.1857\n",
            "Epoch [1/2], Batch [35/134] - Batch Loss: -0.2379\n",
            "Epoch [1/2], Batch [36/134] - Batch Loss: -0.1593\n",
            "Epoch [1/2], Batch [37/134] - Batch Loss: -0.0605\n",
            "Epoch [1/2], Batch [38/134] - Batch Loss: -0.0783\n",
            "Epoch [1/2], Batch [39/134] - Batch Loss: -0.1604\n",
            "Epoch [1/2], Batch [40/134] - Batch Loss: -0.2100\n",
            "Epoch [1/2], Batch [41/134] - Batch Loss: -0.1133\n",
            "Epoch [1/2], Batch [42/134] - Batch Loss: -0.2151\n",
            "Epoch [1/2], Batch [43/134] - Batch Loss: -0.0577\n",
            "Epoch [1/2], Batch [44/134] - Batch Loss: -0.0676\n",
            "Epoch [1/2], Batch [45/134] - Batch Loss: -0.0933\n",
            "Epoch [1/2], Batch [46/134] - Batch Loss: -0.1283\n",
            "Epoch [1/2], Batch [47/134] - Batch Loss: -0.1696\n",
            "Epoch [1/2], Batch [49/134] - Batch Loss: -0.1356\n",
            "Epoch [1/2], Batch [50/134] - Batch Loss: -0.0549\n",
            "Epoch [1/2], Batch [51/134] - Batch Loss: -0.1515\n",
            "Epoch [1/2], Batch [52/134] - Batch Loss: -0.1692\n",
            "Epoch [1/2], Batch [53/134] - Batch Loss: -0.1669\n",
            "Epoch [1/2], Batch [54/134] - Batch Loss: -0.2350\n",
            "Epoch [1/2], Batch [55/134] - Batch Loss: -0.1279\n",
            "Epoch [1/2], Batch [56/134] - Batch Loss: -0.0668\n",
            "Epoch [1/2], Batch [57/134] - Batch Loss: -0.1085\n",
            "Epoch [1/2], Batch [58/134] - Batch Loss: -0.1452\n",
            "Epoch [1/2], Batch [59/134] - Batch Loss: -0.1596\n",
            "Epoch [1/2], Batch [60/134] - Batch Loss: -0.1525\n",
            "Epoch [1/2], Batch [61/134] - Batch Loss: -0.0822\n",
            "Epoch [1/2], Batch [62/134] - Batch Loss: -0.2061\n",
            "Epoch [1/2], Batch [63/134] - Batch Loss: -0.0381\n",
            "Epoch [1/2], Batch [64/134] - Batch Loss: -0.1834\n",
            "Epoch [1/2], Batch [65/134] - Batch Loss: -0.1978\n",
            "Epoch [1/2], Batch [66/134] - Batch Loss: -0.1524\n",
            "Epoch [1/2], Batch [67/134] - Batch Loss: -0.0345\n",
            "Epoch [1/2], Batch [68/134] - Batch Loss: -0.1702\n",
            "Epoch [1/2], Batch [69/134] - Batch Loss: -0.1326\n",
            "Epoch [1/2], Batch [70/134] - Batch Loss: -0.0739\n",
            "Epoch [1/2], Batch [71/134] - Batch Loss: -0.0871\n",
            "Epoch [1/2], Batch [73/134] - Batch Loss: -0.2985\n",
            "Epoch [1/2], Batch [74/134] - Batch Loss: -0.1140\n",
            "Epoch [1/2], Batch [75/134] - Batch Loss: -0.2312\n",
            "Epoch [1/2], Batch [76/134] - Batch Loss: -0.1901\n",
            "Epoch [1/2], Batch [77/134] - Batch Loss: -0.1412\n",
            "Epoch [1/2], Batch [78/134] - Batch Loss: -0.2584\n",
            "Epoch [1/2], Batch [79/134] - Batch Loss: -0.3184\n",
            "Epoch [1/2], Batch [80/134] - Batch Loss: -0.0291\n",
            "Epoch [1/2], Batch [81/134] - Batch Loss: -0.2171\n",
            "Epoch [1/2], Batch [82/134] - Batch Loss: -0.1068\n",
            "Epoch [1/2], Batch [83/134] - Batch Loss: -0.1580\n",
            "Epoch [1/2], Batch [84/134] - Batch Loss: -0.2574\n",
            "Epoch [1/2], Batch [85/134] - Batch Loss: -0.1285\n",
            "Epoch [1/2], Batch [86/134] - Batch Loss: -0.2255\n",
            "Epoch [1/2], Batch [87/134] - Batch Loss: -0.1208\n",
            "Epoch [1/2], Batch [88/134] - Batch Loss: -0.0980\n",
            "Epoch [1/2], Batch [89/134] - Batch Loss: -0.1090\n",
            "Epoch [1/2], Batch [90/134] - Batch Loss: -0.0846\n",
            "Epoch [1/2], Batch [91/134] - Batch Loss: -0.1899\n",
            "Epoch [1/2], Batch [92/134] - Batch Loss: -0.3000\n",
            "Epoch [1/2], Batch [93/134] - Batch Loss: -0.1978\n",
            "Epoch [1/2], Batch [94/134] - Batch Loss: -0.1631\n",
            "Epoch [1/2], Batch [95/134] - Batch Loss: -0.0708\n",
            "Epoch [1/2], Batch [97/134] - Batch Loss: -0.1989\n",
            "Epoch [1/2], Batch [98/134] - Batch Loss: -0.1115\n",
            "Epoch [1/2], Batch [99/134] - Batch Loss: -0.2534\n",
            "Epoch [1/2], Batch [100/134] - Batch Loss: -0.1342\n",
            "Epoch [1/2], Batch [101/134] - Batch Loss: -0.1294\n",
            "Epoch [1/2], Batch [102/134] - Batch Loss: -0.1922\n",
            "Epoch [1/2], Batch [103/134] - Batch Loss: -0.0712\n",
            "Epoch [1/2], Batch [104/134] - Batch Loss: -0.1225\n",
            "Epoch [1/2], Batch [105/134] - Batch Loss: -0.5100\n",
            "Epoch [1/2], Batch [106/134] - Batch Loss: -0.3397\n",
            "Epoch [1/2], Batch [107/134] - Batch Loss: -0.2300\n",
            "Epoch [1/2], Batch [108/134] - Batch Loss: -0.2686\n",
            "Epoch [1/2], Batch [109/134] - Batch Loss: -0.1898\n",
            "Epoch [1/2], Batch [110/134] - Batch Loss: -0.1515\n",
            "Epoch [1/2], Batch [111/134] - Batch Loss: -0.1540\n",
            "Epoch [1/2], Batch [112/134] - Batch Loss: -0.1429\n",
            "Epoch [1/2], Batch [113/134] - Batch Loss: -0.1806\n",
            "Epoch [1/2], Batch [114/134] - Batch Loss: -0.5177\n",
            "Epoch [1/2], Batch [115/134] - Batch Loss: -0.2372\n",
            "Epoch [1/2], Batch [116/134] - Batch Loss: -0.2586\n",
            "Epoch [1/2], Batch [117/134] - Batch Loss: -0.4260\n",
            "Epoch [1/2], Batch [118/134] - Batch Loss: -0.2126\n",
            "Epoch [1/2], Batch [119/134] - Batch Loss: -0.3209\n",
            "Epoch [1/2], Batch [121/134] - Batch Loss: -0.1614\n",
            "Epoch [1/2], Batch [122/134] - Batch Loss: -0.0790\n",
            "Epoch [1/2], Batch [123/134] - Batch Loss: -0.1627\n",
            "Epoch [1/2], Batch [124/134] - Batch Loss: -0.3831\n",
            "Epoch [1/2], Batch [125/134] - Batch Loss: -0.1671\n",
            "Epoch [1/2], Batch [126/134] - Batch Loss: -0.3585\n",
            "Epoch [1/2], Batch [127/134] - Batch Loss: -0.3667\n",
            "Epoch [1/2], Batch [128/134] - Batch Loss: -0.3664\n",
            "Epoch [1/2], Batch [129/134] - Batch Loss: -0.1408\n",
            "Epoch [1/2], Batch [130/134] - Batch Loss: -0.3023\n",
            "Epoch [1/2], Batch [131/134] - Batch Loss: -0.1635\n",
            "Epoch [1/2], Batch [132/134] - Batch Loss: -0.3813\n",
            "Epoch [1/2], Batch [133/134] - Batch Loss: -0.1335\n",
            "Epoch [1/2] - Total Loss: -1395.9747\n",
            "Epoch [2/2], Batch [1/134] - Batch Loss: -0.1336\n",
            "Epoch [2/2], Batch [2/134] - Batch Loss: -0.5895\n",
            "Epoch [2/2], Batch [3/134] - Batch Loss: -0.3030\n",
            "Epoch [2/2], Batch [4/134] - Batch Loss: -0.2115\n",
            "Epoch [2/2], Batch [5/134] - Batch Loss: -0.1263\n",
            "Epoch [2/2], Batch [6/134] - Batch Loss: -0.4022\n",
            "Epoch [2/2], Batch [7/134] - Batch Loss: -0.2865\n",
            "Epoch [2/2], Batch [8/134] - Batch Loss: -0.6085\n",
            "Epoch [2/2], Batch [9/134] - Batch Loss: -0.3479\n",
            "Epoch [2/2], Batch [11/134] - Batch Loss: -0.2178\n",
            "Epoch [2/2], Batch [12/134] - Batch Loss: -0.4024\n",
            "Epoch [2/2], Batch [13/134] - Batch Loss: -0.2018\n",
            "Epoch [2/2], Batch [14/134] - Batch Loss: -0.3551\n",
            "Epoch [2/2], Batch [15/134] - Batch Loss: -0.3016\n",
            "Epoch [2/2], Batch [16/134] - Batch Loss: -0.6400\n",
            "Epoch [2/2], Batch [17/134] - Batch Loss: -0.4126\n",
            "Epoch [2/2], Batch [18/134] - Batch Loss: -0.2266\n",
            "Epoch [2/2], Batch [19/134] - Batch Loss: -0.4193\n",
            "Epoch [2/2], Batch [20/134] - Batch Loss: -0.4715\n",
            "Epoch [2/2], Batch [21/134] - Batch Loss: -0.6372\n",
            "Epoch [2/2], Batch [22/134] - Batch Loss: -0.6782\n",
            "Epoch [2/2], Batch [23/134] - Batch Loss: -0.6874\n",
            "Epoch [2/2], Batch [24/134] - Batch Loss: -0.2389\n",
            "Epoch [2/2], Batch [25/134] - Batch Loss: -0.6722\n",
            "Epoch [2/2], Batch [26/134] - Batch Loss: -0.5791\n",
            "Epoch [2/2], Batch [27/134] - Batch Loss: -0.3134\n",
            "Epoch [2/2], Batch [28/134] - Batch Loss: -0.3875\n",
            "Epoch [2/2], Batch [29/134] - Batch Loss: -0.6865\n",
            "Epoch [2/2], Batch [30/134] - Batch Loss: -0.3662\n",
            "Epoch [2/2], Batch [31/134] - Batch Loss: -0.3791\n",
            "Epoch [2/2], Batch [32/134] - Batch Loss: -0.4990\n",
            "Epoch [2/2], Batch [33/134] - Batch Loss: -0.5283\n",
            "Epoch [2/2], Batch [35/134] - Batch Loss: -0.4446\n",
            "Epoch [2/2], Batch [36/134] - Batch Loss: -0.3046\n",
            "Epoch [2/2], Batch [37/134] - Batch Loss: -0.7329\n",
            "Epoch [2/2], Batch [38/134] - Batch Loss: -0.5897\n",
            "Epoch [2/2], Batch [39/134] - Batch Loss: -1.3859\n",
            "Epoch [2/2], Batch [40/134] - Batch Loss: -1.2637\n",
            "Epoch [2/2], Batch [41/134] - Batch Loss: -0.6387\n",
            "Epoch [2/2], Batch [42/134] - Batch Loss: -0.7845\n",
            "Epoch [2/2], Batch [43/134] - Batch Loss: -0.9075\n",
            "Epoch [2/2], Batch [44/134] - Batch Loss: -0.6964\n",
            "Epoch [2/2], Batch [45/134] - Batch Loss: -0.6205\n",
            "Epoch [2/2], Batch [46/134] - Batch Loss: -0.3578\n",
            "Epoch [2/2], Batch [47/134] - Batch Loss: -0.7517\n",
            "Epoch [2/2], Batch [48/134] - Batch Loss: -1.5027\n",
            "Epoch [2/2], Batch [49/134] - Batch Loss: -1.0442\n",
            "Epoch [2/2], Batch [50/134] - Batch Loss: -1.2231\n",
            "Epoch [2/2], Batch [51/134] - Batch Loss: -1.0947\n",
            "Epoch [2/2], Batch [52/134] - Batch Loss: -1.0741\n",
            "Epoch [2/2], Batch [53/134] - Batch Loss: -0.8425\n",
            "Epoch [2/2], Batch [54/134] - Batch Loss: -0.3675\n",
            "Epoch [2/2], Batch [55/134] - Batch Loss: -1.5219\n",
            "Epoch [2/2], Batch [56/134] - Batch Loss: -0.7875\n",
            "Epoch [2/2], Batch [57/134] - Batch Loss: -0.6558\n",
            "Epoch [2/2], Batch [59/134] - Batch Loss: -0.8403\n",
            "Epoch [2/2], Batch [60/134] - Batch Loss: -1.8924\n",
            "Epoch [2/2], Batch [61/134] - Batch Loss: -1.4778\n",
            "Epoch [2/2], Batch [62/134] - Batch Loss: -1.3185\n",
            "Epoch [2/2], Batch [63/134] - Batch Loss: -1.2513\n",
            "Epoch [2/2], Batch [64/134] - Batch Loss: -1.8680\n",
            "Epoch [2/2], Batch [65/134] - Batch Loss: -0.9738\n",
            "Epoch [2/2], Batch [66/134] - Batch Loss: -1.8602\n",
            "Epoch [2/2], Batch [67/134] - Batch Loss: -1.9499\n",
            "Epoch [2/2], Batch [68/134] - Batch Loss: -1.0106\n",
            "Epoch [2/2], Batch [69/134] - Batch Loss: -2.1069\n",
            "Epoch [2/2], Batch [70/134] - Batch Loss: -1.4627\n",
            "Epoch [2/2], Batch [71/134] - Batch Loss: -1.8944\n",
            "Epoch [2/2], Batch [72/134] - Batch Loss: -2.1267\n",
            "Epoch [2/2], Batch [73/134] - Batch Loss: -2.7675\n",
            "Epoch [2/2], Batch [74/134] - Batch Loss: -2.6610\n",
            "Epoch [2/2], Batch [75/134] - Batch Loss: -1.9675\n",
            "Epoch [2/2], Batch [76/134] - Batch Loss: -2.3517\n",
            "Epoch [2/2], Batch [77/134] - Batch Loss: -2.3831\n",
            "Epoch [2/2], Batch [78/134] - Batch Loss: -2.1134\n",
            "Epoch [2/2], Batch [79/134] - Batch Loss: -2.5428\n",
            "Epoch [2/2], Batch [80/134] - Batch Loss: -1.6873\n",
            "Epoch [2/2], Batch [81/134] - Batch Loss: -3.5658\n",
            "Epoch [2/2], Batch [83/134] - Batch Loss: -3.8294\n",
            "Epoch [2/2], Batch [84/134] - Batch Loss: -3.5849\n",
            "Epoch [2/2], Batch [85/134] - Batch Loss: -3.4110\n",
            "Epoch [2/2], Batch [86/134] - Batch Loss: -3.0581\n",
            "Epoch [2/2], Batch [87/134] - Batch Loss: -2.5647\n",
            "Epoch [2/2], Batch [88/134] - Batch Loss: -2.5277\n",
            "Epoch [2/2], Batch [89/134] - Batch Loss: -3.8221\n",
            "Epoch [2/2], Batch [90/134] - Batch Loss: -3.0486\n",
            "Epoch [2/2], Batch [91/134] - Batch Loss: -3.7089\n",
            "Epoch [2/2], Batch [92/134] - Batch Loss: -4.5049\n",
            "Epoch [2/2], Batch [93/134] - Batch Loss: -3.9788\n",
            "Epoch [2/2], Batch [94/134] - Batch Loss: -3.2065\n",
            "Epoch [2/2], Batch [95/134] - Batch Loss: -3.9054\n",
            "Epoch [2/2], Batch [96/134] - Batch Loss: -4.5951\n",
            "Epoch [2/2], Batch [97/134] - Batch Loss: -4.3479\n",
            "Epoch [2/2], Batch [98/134] - Batch Loss: -3.7876\n",
            "Epoch [2/2], Batch [99/134] - Batch Loss: -5.5218\n",
            "Epoch [2/2], Batch [100/134] - Batch Loss: -4.9889\n",
            "Epoch [2/2], Batch [101/134] - Batch Loss: -3.8774\n",
            "Epoch [2/2], Batch [102/134] - Batch Loss: -4.4539\n",
            "Epoch [2/2], Batch [103/134] - Batch Loss: -6.2787\n",
            "Epoch [2/2], Batch [104/134] - Batch Loss: -4.8700\n",
            "Epoch [2/2], Batch [105/134] - Batch Loss: -5.8027\n",
            "Epoch [2/2], Batch [107/134] - Batch Loss: -5.8579\n",
            "Epoch [2/2], Batch [108/134] - Batch Loss: -5.8736\n",
            "Epoch [2/2], Batch [109/134] - Batch Loss: -6.4599\n",
            "Epoch [2/2], Batch [110/134] - Batch Loss: -6.9615\n",
            "Epoch [2/2], Batch [111/134] - Batch Loss: -7.4869\n",
            "Epoch [2/2], Batch [112/134] - Batch Loss: -5.6376\n",
            "Epoch [2/2], Batch [113/134] - Batch Loss: -6.6759\n",
            "Epoch [2/2], Batch [114/134] - Batch Loss: -7.7160\n",
            "Epoch [2/2], Batch [115/134] - Batch Loss: -6.4251\n",
            "Epoch [2/2], Batch [116/134] - Batch Loss: -5.2572\n",
            "Epoch [2/2], Batch [117/134] - Batch Loss: -7.8933\n",
            "Epoch [2/2], Batch [118/134] - Batch Loss: -7.9743\n",
            "Epoch [2/2], Batch [119/134] - Batch Loss: -6.9316\n",
            "Epoch [2/2], Batch [120/134] - Batch Loss: -5.9580\n",
            "Epoch [2/2], Batch [121/134] - Batch Loss: -10.1978\n",
            "Epoch [2/2], Batch [122/134] - Batch Loss: -8.8111\n",
            "Epoch [2/2], Batch [123/134] - Batch Loss: -8.4432\n",
            "Epoch [2/2], Batch [124/134] - Batch Loss: -10.3326\n",
            "Epoch [2/2], Batch [125/134] - Batch Loss: -10.4616\n",
            "Epoch [2/2], Batch [126/134] - Batch Loss: -10.3550\n",
            "Epoch [2/2], Batch [127/134] - Batch Loss: -12.1132\n",
            "Epoch [2/2], Batch [128/134] - Batch Loss: -10.8947\n",
            "Epoch [2/2], Batch [129/134] - Batch Loss: -8.1428\n",
            "Epoch [2/2], Batch [131/134] - Batch Loss: -11.1743\n",
            "Epoch [2/2], Batch [132/134] - Batch Loss: -10.3092\n",
            "Epoch [2/2], Batch [133/134] - Batch Loss: -8.7525\n",
            "Epoch [2/2] - Total Loss: -24559.9309\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Performance\n",
        "test_acc = evaluation(unlearned_model, test_dataloader)\n",
        "unseen_acc = evaluation(unlearned_model, unseen_dataloader)\n",
        "mia = cal_mia(unlearned_model.cuda())\n",
        "print(f'Test Acc: {test_acc}')\n",
        "print(f'Unseen Acc: {unseen_acc}')\n",
        "print(f'MIA: {mia}')\n",
        "print(f'Final Score: {(test_acc[\"Acc\"] + (1 - abs(mia[\"MIA\"] - 0.5) * 2)) / 2}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1ay78Zb1M09",
        "outputId": "5275c116-2542-4d6e-a776-f4c9bbac65a2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Test]\n",
            "[Batch: 1] running test loss: 0.1612485945224762, running test accuracy: 0.5, running top-2 accuracy: 0.65625\n",
            "[Batch: 20] running test loss: 0.21013431623578072, running test accuracy: 0.39296875, running top-2 accuracy: 0.59375\n",
            "test loss: 0.2289872773674264, accuracy: 0.3931124106562703, top-2 accuracy: 0.5925925925925926\n",
            "elapsed time: 1.6150825023651123\n",
            "[Test]\n",
            "[Batch: 1] running test loss: 0.13831579685211182, running test accuracy: 0.453125, running top-2 accuracy: 0.71875\n",
            "[Batch: 20] running test loss: 0.18861337676644324, running test accuracy: 0.43203125, running top-2 accuracy: 0.61796875\n",
            "test loss: 0.19546493697673717, accuracy: 0.44148936170212766, top-2 accuracy: 0.6223404255319149\n",
            "elapsed time: 1.6341218948364258\n",
            "Test Acc: {'Loss': 0.2289872773674264, 'Acc': 0.3931124106562703, 'Top-2 Acc': 0.5925925925925926}\n",
            "Unseen Acc: {'Loss': 0.19546493697673717, 'Acc': 0.44148936170212766, 'Top-2 Acc': 0.6223404255319149}\n",
            "MIA: {'MIA': 0.5514950166112957, 'Forgeting Score': 0.05149501661129574}\n",
            "Final Score: 0.6450611887168394\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}